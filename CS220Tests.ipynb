{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUmATrkxsjm4mPTf2MJ9Xf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezpogue/CS220Project/blob/main/CS220Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f42GKC-9W1h9",
        "outputId": "2c16a196-0b09-4913-a423-45af7f34c58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS220Project'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone  https://github.com/ezpogue/CS220Project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Information\n",
        "\n",
        "This notebook is where we've been testing various methods of implementing approximate arithmetic into our Tensorflow neural networks. Almost all of the code in this notebook is either from the Tensorflow documentation directly or from ChatGPT/Google Bard."
      ],
      "metadata": {
        "id": "bo8jUxy3X0i1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up"
      ],
      "metadata": {
        "id": "qjltDv1mYVIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up tensorflow"
      ],
      "metadata": {
        "id": "7dXYOOeRYSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "id": "IVSDlxeXXxxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset, we are using the MNIST fashion dataset"
      ],
      "metadata": {
        "id": "X0CjYVBEYZt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "mG3ofreWYa1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom functions"
      ],
      "metadata": {
        "id": "yaGNTa9JZI7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small custom matmul function to verify custom functions work"
      ],
      "metadata": {
        "id": "ihAXRI7GY5kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference\n",
        "def custom_matmul(a, b, transpose_a=False, transpose_b=False):\n",
        "  return tf.matmul(a,b)"
      ],
      "metadata": {
        "id": "VRQyPwW-Yzqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 1: This uses a custom matmul, which calls a custom reduce_sum, which calls a custom sum. Compared to the future attempts, this is a really roundabout way of achieving what we want, and it doesn't even work properly. This is the point we switched from ChatGPT generated functions to Google Bard generated functions."
      ],
      "metadata": {
        "id": "5vIRiF0iZ6rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def custom_matmul(a, b):\n",
        "    # Ensure the inner dimensions match for matrix multiplication\n",
        "    assert a.shape[-1] == b.shape[0], \"Inner dimensions do not match for matrix multiplication\"\n",
        "\n",
        "    # Get shapes of input matrices\n",
        "    a_shape = tf.shape(a)\n",
        "    b_shape = tf.shape(b)\n",
        "\n",
        "    # Reshape the matrices to 2D\n",
        "    a_reshaped = tf.reshape(a, [-1, a_shape[-1]])\n",
        "    b_reshaped = tf.reshape(b, [b_shape[0], -1])\n",
        "\n",
        "    # Transpose matrix b\n",
        "    b_transposed = tf.transpose(b_reshaped)\n",
        "\n",
        "    # Perform element-wise multiplication\n",
        "    elementwise_product = tf.expand_dims(a_reshaped, 2) * tf.expand_dims(b_transposed, 0)\n",
        "\n",
        "    # Sum along the inner dimension to get the result\n",
        "    result = custom_reduce_sum(elementwise_product, axis=1)\n",
        "\n",
        "    # Reshape the result back to the original shape\n",
        "    result = tf.reshape(result, tf.concat([a_shape[:-1], b_shape[1:]], axis=0))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def custom_reduce_sum(input_tensor, axis=None, keepdims=False):\n",
        "    # Convert axis to a list for more flexible handling\n",
        "    if axis is None:\n",
        "        axis = list(range(len(input_tensor.shape)))\n",
        "    elif not isinstance(axis, list):\n",
        "        axis = [axis]\n",
        "\n",
        "    # Determine the dimensions to reduce\n",
        "    reduce_dims = []\n",
        "    for a in axis:\n",
        "        reduce_dims.append(input_tensor.shape[a])\n",
        "\n",
        "    # Perform reduction by manually summing along the specified axis\n",
        "    result = input_tensor.numpy()  # Convert to NumPy array for simplicity\n",
        "\n",
        "    for dim in sorted(reduce_dims, reverse=True):\n",
        "        result = custom_sum(result, axis=dim, keepdims=keepdims)\n",
        "\n",
        "    return tf.constant(result)\n",
        "\n",
        "def custom_sum(array, axis=None, keepdims=False):\n",
        "    if axis is None:\n",
        "        # Sum over all elements if axis is None\n",
        "        result = 0\n",
        "        for element in np.nditer(array):\n",
        "            result += element\n",
        "        return np.array(result) if keepdims else result\n",
        "\n",
        "    # If axis is specified, manually sum along the specified axis\n",
        "    axis = axis if isinstance(axis, tuple) else (axis,)\n",
        "    reduce_dims = sorted(axis, reverse=True)\n",
        "\n",
        "    result = array.copy()\n",
        "    for dim in reduce_dims:\n",
        "        # Manually sum along the specified axis\n",
        "        for i in range(result.shape[dim]):\n",
        "            result = np.add.reduceat(result, indices=i, axis=dim, keepdims=True)\n",
        "\n",
        "    return result if keepdims else np.squeeze(result)"
      ],
      "metadata": {
        "id": "bdAUWTfsbU2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 2: Basically manually doing matrix multiplication with the naive method. This implementation breaks in the update step, as sum sometimes becomes an empty list. We are not sure why this happens"
      ],
      "metadata": {
        "id": "oLV2CMJbZeYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Almost works\n",
        "#Sometimes sum = [] which breaks it\n",
        "def custom_matmul(a, b, transpose_a=False, transpose_b=False):\n",
        "  \"\"\"\n",
        "  Performs matrix multiplication with optional transpositions.\n",
        "\n",
        "  Args:\n",
        "    a: Tensor of shape (m, n).\n",
        "    b: Tensor of shape (n, p).\n",
        "    transpose_a: Boolean, whether to transpose a before multiplication.\n",
        "    transpose_b: Boolean, whether to transpose b before multiplication.\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape (m, p).\n",
        "  \"\"\"\n",
        "\n",
        "  a = tf.transpose(a) if transpose_a else a\n",
        "  b = tf.transpose(b) if transpose_b else b\n",
        "\n",
        "  #a = tf.cast(a, dtype=tf.float32)\n",
        "  #b = tf.cast(b, dtype=tf.float32)\n",
        "\n",
        "  m, n, p = tf.shape(a)[0], tf.shape(a)[1], tf.shape(b)[1]\n",
        "  c = tf.zeros(shape=(m, p))\n",
        "\n",
        "  for i in range(m):\n",
        "    for j in range(p):\n",
        "      sum = 0.0\n",
        "      for k in range(n):\n",
        "        sum += a[i, k] * b[k, j]\n",
        "      updated_tensor = tf.tensor_scatter_nd_update(c, updates=sum, indices=[i,j], )\n",
        "  return c"
      ],
      "metadata": {
        "id": "jhyZPXfsZfWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 3: Actually does work, but no approximation has been introduced. It basically does what matmul() does but slower"
      ],
      "metadata": {
        "id": "d5Qyh2l3aCZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Works but doesn't really tell you anything\n",
        "\n",
        "def custom_matmul(a, b, transpose_a=False, transpose_b=False):\n",
        "  \"\"\"\n",
        "  Performs matrix multiplication with optional transpositions.\n",
        "\n",
        "  Args:\n",
        "    a: Tensor of shape (m, n).\n",
        "    b: Tensor of shape (n, p).\n",
        "    transpose_a: Boolean, whether to transpose a before multiplication.\n",
        "    transpose_b: Boolean, whether to transpose b before multiplication.\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape (m, p).\n",
        "  \"\"\"\n",
        "\n",
        "  a = tf.transpose(a) if transpose_a else a\n",
        "  b = tf.transpose(b) if transpose_b else b\n",
        "\n",
        "  # Einsum summation for efficient matrix multiplication\n",
        "  #return tf.einsum(\"mi,jk->mj\", a, b)\n",
        "\n",
        "  # Alternative using element-wise multiplication and reduction\n",
        "  c = a[..., None] * b[None, ...]\n",
        "  return tf.reduce_sum(c, axis=-1)"
      ],
      "metadata": {
        "id": "P25n0ruEZ4WD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 4: Same as attempt 3, but with a custom reduce_sum function. This one breaks and we don't know why"
      ],
      "metadata": {
        "id": "20kEUhNsaNfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DOES NOT WORK\n",
        "#Not sure the actual issue\n",
        "\n",
        "def custom_matmul(a, b, transpose_a=False, transpose_b=False):\n",
        "  \"\"\"\n",
        "  Performs matrix multiplication with optional transpositions.\n",
        "\n",
        "  Args:\n",
        "    a: Tensor of shape (m, n).\n",
        "    b: Tensor of shape (n, p).\n",
        "    transpose_a: Boolean, whether to transpose a before multiplication.\n",
        "    transpose_b: Boolean, whether to transpose b before multiplication.\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape (m, p).\n",
        "  \"\"\"\n",
        "\n",
        "  a = tf.transpose(a) if transpose_a else a\n",
        "  b = tf.transpose(b) if transpose_b else b\n",
        "\n",
        "  # Einsum summation for efficient matrix multiplication\n",
        "  #return tf.einsum(\"mi,jk->mj\", a, b)\n",
        "\n",
        "  # Alternative using element-wise multiplication and reduction\n",
        "  c = a[..., None] * b[None, ...]\n",
        "  return tf.custom_reduce_sum(c, axis=-1)\n",
        "\n",
        "\n",
        "#Does not work\n",
        "def custom_reduce_sum(x, axis=None):\n",
        "  sum_val = 0.0\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      sum_val += x[i, j]\n",
        "  # Adapt the loop based on your desired reduction axes\n",
        "  return sum_val"
      ],
      "metadata": {
        "id": "0kc8dW1xaTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 5: Same as attempt 3, but with an approximate multiplication function used instead of normal multiplication. This one doesn't work due to the Tensor to string conversion breaking"
      ],
      "metadata": {
        "id": "VD98N3PDaiMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DOES NOT WORK\n",
        "#Tried approximating tensor multiplication instead of exact\n",
        "\n",
        "\n",
        "def custom_matmul(a, b, transpose_a=False, transpose_b=False):\n",
        "  \"\"\"\n",
        "  Performs matrix multiplication with optional transpositions.\n",
        "\n",
        "  Args:\n",
        "    a: Tensor of shape (m, n).\n",
        "    b: Tensor of shape (n, p).\n",
        "    transpose_a: Boolean, whether to transpose a before multiplication.\n",
        "    transpose_b: Boolean, whether to transpose b before multiplication.\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape (m, p).\n",
        "  \"\"\"\n",
        "\n",
        "  a = tf.transpose(a) if transpose_a else a\n",
        "  b = tf.transpose(b) if transpose_b else b\n",
        "\n",
        "  # Einsum summation for efficient matrix multiplication\n",
        "  #return tf.einsum(\"mi,jk->mj\", a, b)\n",
        "\n",
        "  # Alternative using element-wise multiplication and reduction\n",
        "  c = approximate_tensor_multiplier(a[..., None], b[None, ...])\n",
        "  return tf.reduce_sum(c, axis=-1)\n",
        "\n",
        "\n",
        "def approximate_tensor_multiplier(a, b, n_bits=4):\n",
        "  \"\"\"\n",
        "  Approximates multiplication of two tensors using a simplified circuit-like approach.\n",
        "\n",
        "  Args:\n",
        "    a: Tensor of shape (m, n).\n",
        "    b: Tensor of shape (n, p).\n",
        "    n_bits: Number of bits for binary representation (higher values improve accuracy).\n",
        "\n",
        "  Returns:\n",
        "    Tensor of shape (m, p) representing the approximated product.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert integer elements to binary strings with padding\n",
        "  a_bin = tf.strings.as_string(tf.math.bitwise.bitwise_and(a, 2**n_bits - 1), base=2, padlen=n_bits)\n",
        "  b_bin = tf.strings.as_string(tf.math.bitwise.bitwise_and(b, 2**n_bits - 1), base=2, padlen=n_bits)\n",
        "\n",
        "  # Initialize partial product tensor with zeros\n",
        "  product = tf.zeros_like(tf.matmul(a, b))\n",
        "\n",
        "  # Iterate over columns of the second tensor\n",
        "  for i in range(b.shape[1]):\n",
        "    # Extract current bit column from the second tensor\n",
        "    current_bit = tf.strings.substr(b_bin, i, 1)\n",
        "    current_bit = tf.cast(tf.strings.to_number(current_bit), tf.bool)\n",
        "\n",
        "    # Add shifted first tensor elements based on the current bit\n",
        "    shifted_a = tf.bitwise.left_shift(a[:, i], tf.range(n_bits, dtype=tf.int32)[::-1])\n",
        "    product += tf.where(current_bit, shifted_a, tf.zeros_like(shifted_a))\n",
        "\n",
        "  # Apply sign based on original signs\n",
        "  sign_matrix = tf.ones_like(product)\n",
        "  condition = tf.math.logical_and(tf.math.reduce_any(tf.math.less(a, 0), axis=1), tf.math.reduce_any(tf.math.less(b, 0), axis=0))\n",
        "  sign_matrix = tf.where(condition, -1.0 * sign_matrix, sign_matrix)\n",
        "  product *= sign_matrix\n",
        "\n",
        "  return product"
      ],
      "metadata": {
        "id": "5P9di54wahjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models"
      ],
      "metadata": {
        "id": "yN7WNzlEYpPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default neural network from the documentation"
      ],
      "metadata": {
        "id": "TuQ2TX0QYruY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "uwMPvbhiYnX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as default model, but replace the first dense layer with a custom layer. This one uses a rounded matrix multiplication using OR. This one missed the mark a bit since it calls matmul anyways"
      ],
      "metadata": {
        "id": "E7RYg68ic1dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layer with custom OR operation\n",
        "class CustomLogicLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(CustomLogicLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Define trainable weights for the layer\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Custom OR operation: Check if the result of the matrix multiplication\n",
        "        # is greater than 0 or if the bias term is greater than 0\n",
        "        result = tf.math.logical_or(tf.matmul(inputs, self.w) > 0, self.b > 0)\n",
        "\n",
        "        # Convert boolean values to float32 (0.0 or 1.0)\n",
        "        result = tf.cast(result, tf.float32)\n",
        "        return result\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  CustomLogicLayer(units=128),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "Pn-Ks21Kc02c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as above but with just matrix multiplication"
      ],
      "metadata": {
        "id": "unKiolzndDIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layer with stock operation\n",
        "class CustomLogicLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(CustomLogicLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Define trainable weights for the layer\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Custom OR operation: Check if the result of the matrix multiplication\n",
        "        # is greater than 0 or if the bias term is greater than 0\n",
        "        result = tf.matmul(inputs, self.w)\n",
        "\n",
        "        # Convert boolean values to float32 (0.0 or 1.0)\n",
        "        result = tf.cast(result, tf.float32)\n",
        "        return result\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  CustomLogicLayer(units=128),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "QMY3Ft3DdCwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as above but with matmul replaced with custom_matmul. It uses whichever function we last compiled from the earlier section."
      ],
      "metadata": {
        "id": "8c6yogN8dT33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layer with custom matmul operation\n",
        "class CustomLogicLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(CustomLogicLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Define trainable weights for the layer\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Custom OR operation: Check if the result of the matrix multiplication\n",
        "        # is greater than 0 or if the bias term is greater than 0\n",
        "        result = custom_matmul(inputs, self.w)\n",
        "\n",
        "        # Convert boolean values to float32 (0.0 or 1.0)\n",
        "        result = tf.cast(result, tf.float32)\n",
        "        return result\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  CustomLogicLayer(units=128),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "gFutua5VdUP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Compilation"
      ],
      "metadata": {
        "id": "7g6FA8M7dkWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create logbits and convert to probabilities. Not entirely sure how this works\n",
        "\n"
      ],
      "metadata": {
        "id": "dTkuxog6dut_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "id": "oSozGFsndkoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "id": "51-gc-O5dmBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function"
      ],
      "metadata": {
        "id": "FzqH9aUMdqex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "bHgAtIhEdmQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "id": "U4LTuDcidmjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "eh1YHE9WeH7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bb9jAusueIiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation"
      ],
      "metadata": {
        "id": "5ty_Bbp4eP6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit model to our training set. Default epochs is 5"
      ],
      "metadata": {
        "id": "yc-mTDD1eVy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "oThTZgoteRQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model. The true default model comes out to about 97% accuracy"
      ],
      "metadata": {
        "id": "UWFTymrqecKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "id": "dVpL7Q-oeeDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}